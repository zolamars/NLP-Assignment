{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5n8BVh_Yr9Q"
      },
      "source": [
        "# Natural Language Processing (NLP) Assignment\n",
        "This assignment will guide you through the basic concepts of Natural Language Processing including:\n",
        "- Text preprocessing\n",
        "- Tokenization and N-grams\n",
        "- Named Entity Recognition (NER)\n",
        "- Converting text into numbers (vectorization)\n",
        "- Word embeddings (for experienced learners)\n",
        "\n",
        "You can run and modify the code cells below to complete the tasks."
      ],
      "id": "Z5n8BVh_Yr9Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCF3KD-EYr9S"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "iCF3KD-EYr9S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEsF_R2lYr9U"
      },
      "source": [
        "## 1. Text Preprocessing\n",
        "Clean the following text by converting it to lowercase, removing punctuation and stop words."
      ],
      "id": "DEsF_R2lYr9U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP7-LkEdYr9U"
      },
      "outputs": [],
      "source": [
        "import string # Required for punctuation handling\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is a fascinating field. It combines linguistics and computer science!\"\n",
        "\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize into individual words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Load English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Remove punctuation and stopwords\n",
        "    cleaned_tokens = [\n",
        "        token\n",
        "        for token in tokens\n",
        "        if token not in stop_words and token not in string.punctuation\n",
        "    ]\n",
        "    return cleaned_tokens\n",
        "\n",
        "cleaned_tokens = preprocess(text)\n",
        "print(\"Cleaned Tokens:\", cleaned_tokens)"
      ],
      "id": "HP7-LkEdYr9U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGccz4YaYr9U"
      },
      "source": [
        "## 2. Tokenization and N-grams\n",
        "Generate bigrams (2-grams) from the cleaned tokens."
      ],
      "id": "cGccz4YaYr9U"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "FoIq0pfbYr9V",
        "outputId": "f347fb2d-7121-4b2e-fef1-80364b177382"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cleaned_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d19a6dafc5d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Generate bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bigrams:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cleaned_tokens' is not defined"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = list(ngrams(cleaned_tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)"
      ],
      "id": "FoIq0pfbYr9V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPkUW6XbYr9V"
      },
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "Use spaCy to perform NER on a new sentence."
      ],
      "id": "rPkUW6XbYr9V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpwmRm8zYr9V"
      },
      "outputs": [],
      "source": [
        "# Example sentence\n",
        "sentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract entities\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")"
      ],
      "id": "vpwmRm8zYr9V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpyGaIFpYr9V"
      },
      "source": [
        "## 4. Converting Text to Numbers\n",
        "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
      ],
      "id": "jpyGaIFpYr9V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKlvJ-EeYr9W"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Natural language processing is a part of AI.\",\n",
        "    \"AI is the future.\"\n",
        "]\n",
        "# Initialize and fit the vectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform(sentences)\n",
        "\n",
        "# Show results\n",
        "print(\"Count Vectorizer Output:\")\n",
        "print(\"Vocabulary:\", count_vec.get_feature_names_out())\n",
        "print(\"Matrix:\\n\", X_count.toarray())\n",
        "\n",
        "# Initialize and fit the vectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
        "\n",
        "# Show results\n",
        "print(\"\\nTF-IDF Vectorizer Output:\")\n",
        "print(\"Vocabulary:\", tfidf_vec.get_feature_names_out())\n",
        "print(\"Matrix:\\n\", X_tfidf.toarray())"
      ],
      "id": "jKlvJ-EeYr9W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH4UNmZxYr9W"
      },
      "source": [
        "## 5. Word Embeddings (Advanced)\n",
        "Use spaCy to get word vectors (embeddings) for given words."
      ],
      "id": "BH4UNmZxYr9W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoJQ7aDjYr9W"
      },
      "outputs": [],
      "source": [
        "# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md\n",
        "# Uncomment below to install and load the medium model if needed.\n",
        "# !python -m spacy download en_core_web_md\n",
        "# nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Load the medium model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Get vector for the word \"machine\"\n",
        "word = nlp(\"machine\")[0]\n",
        "print(\"Vector for 'machine':\\n\", word.vector)"
      ],
      "id": "VoJQ7aDjYr9W"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}